
# Fine-Tuning Lightweight Stable Diffusion for Image-to-Image Generation

This repository demonstrates how to fine-tune a lightweight Stable Diffusion model for image-to-image generation. The project consists of two primary notebooks:

1. **Image Generation Notebook**: This notebook uses a pre-trained Stable Diffusion model to generate new images based on the CIFAR-10 dataset.
   - [Link to Kaggle Notebook](https://www.kaggle.com/code/isratjahankhan/image-generation-notebook)
2. **Fine-Tuning Notebook**: This notebook fine-tunes the Stable Diffusion model on custom data to enhance its image generation capabilities.

## Requirements

To run the notebooks in this repository, the following software and libraries are required:

- **Python 3.7+**
- **Google Colab** (recommended for cloud execution) or **Kaggle**
- **GPU** (recommended for faster execution, though the model can also run on CPU)
- The following Python libraries:
  - `torch` (for tensor operations and model handling)
  - `torchvision` (for dataset manipulation and transformations)
  - `diffusers` (for working with Stable Diffusion models)
  - `PIL` (for image handling and transformations)
  - `google.colab` (for mounting Google Drive in Colab)

### Installation

If running locally or on Colab environment, install the required libraries using `pip`.

## Setup

### 1. Mount Google Drive (In case of using Google Colab)

The images generated by the model are saved to Google Drive for easy access. We will need to mount Google Drive in Google Colab to store the output images.

### 2. Device Setup

The notebooks are designed to automatically detect whether a GPU or CPU is available. If a GPU is available, it will be used for faster processing.

## Workflow

### 1. Image Generation Notebook

This notebook focuses on generating new images based on the CIFAR-10 dataset by leveraging a pre-trained Stable Diffusion model.

- **Model**: The model used is a lightweight version of Stable Diffusion (`stabilityai/stable-diffusion-2-1-base`), which is optimized for faster image generation while maintaining high-quality results.
- **Dataset**: The CIFAR-10 dataset is used, consisting of 60,000 32x32 color images across 10 classes. Images are resized to 512x512 resolution for generation.
- **Generation Process**: A fixed prompt (e.g., "A realistic photo") is used to generate images based on the original CIFAR-10 images, with parameters such as `strength` and `guidance_scale` to control the generation.
- **Output**: The generated images are saved in a directory in Google Drive or Kaggle's output directory, and the labels from the CIFAR-10 dataset are displayed for reference.

### 2. Fine-Tuning Notebook

This notebook involves fine-tuning the pre-trained Stable Diffusion model on a custom dataset to improve its image generation capabilities.

#### Fine-Tuning Process

In this step, the pre-trained Stable Diffusion model is adapted to generate images that more closely align with the characteristics of a custom dataset, in this case, the CIFAR-10 dataset.

**Key Steps Involved:**

1. **Model Initialization**: 
   - The pre-trained Stable Diffusion model (`stabilityai/stable-diffusion-2-1-base`) is loaded into the notebook. This model is optimized for faster image generation while maintaining high-quality results.
   - The model is moved to the appropriate device (GPU or CPU) for training.

2. **Dataset Preparation**: 
   - The CIFAR-10 dataset is used for fine-tuning. Custom transformations are applied to resize the images to 512x512 pixels and normalize them according to the pre-trained model's requirements. 
   - A `DataLoader` is created to batch the images, shuffle the dataset, and load data in parallel using multiple workers.

3. **Freezing Pre-Trained Layers**: 
   - Some layers of the model are frozen to retain the knowledge from the pre-trained model while fine-tuning the later layers to learn specific features from the new dataset. This approach helps prevent overfitting and speeds up the training process.

4. **Training and Optimization**: 
   - The model is fine-tuned using the AdamW optimizer, which is ideal for fine-tuning large models. The learning rate is set to a low value (1e-5) to ensure gradual learning and avoid overfitting.
   - The training process runs for a specified number of epochs (default: 10) where the model learns to generate images based on the CIFAR-10 dataset.
   
5. **Image Generation**:
   - During each epoch, the model generates images in two ways:
     - **Text-to-Image**: The model generates images from text prompts (e.g., "A high-quality, detailed image of a {class_name}").
     - **Image-to-Image**: The model generates enhanced versions of input images using the CIFAR-10 images as the base, modified by a corresponding text prompt (e.g., "Enhanced, photorealistic version of a {class_name}").
   - The generated images are saved for each epoch, providing a progression of improvements in image quality.

6. **Error Handling**: 
   - Comprehensive error handling is implemented throughout the fine-tuning process to manage issues that may arise during model loading, data processing, or image generation.

**Output**: After fine-tuning, the model should generate higher-quality images that are more aligned with the features of the custom dataset.

## Example Usage

After running the Image Generation Notebook, you can download and view the generated images saved in Google Drive. For the Fine-Tuning Notebook, follow these steps:

1. Use the CIFAR-10 dataset, which is already included and loaded in the notebook.
2. Run the fine-tuning script to adapt the pre-trained Stable Diffusion model on the CIFAR-10 dataset.
3. Save the fine-tuned model and use it for generating improved images based on the CIFAR-10 dataset.

## Output Images

Here are some of the generated images from the first notebook (**Image Generation Notebook**):

![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_2.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_5.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_8.jpg?raw=true)

Here are some of the generated images from the second notebook (**Fine-Tuning Notebook**):

![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0_method1.png?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0_method2.png?raw=true)



**Note**: The fine-tuning process described here does not include any advanced steps and is currently under construction. Additional features and improvements will be added in future updates.

