
# Fine-Tuning Lightweight Stable Diffusion for Image-to-Image Generation

This repository demonstrates how to fine-tune a lightweight Stable Diffusion model for image-to-image generation. The project consists of two primary notebooks:

1. **Image Generation Notebook**: This notebook uses a pre-trained Stable Diffusion model to generate new images based on the CIFAR-10 dataset.
   - [Link to Kaggle Notebook](https://www.kaggle.com/code/isratjahankhan/image-generation-notebook)
2. **Fine-Tuning Notebook**: This notebook fine-tunes the Stable Diffusion model on custom data to enhance its image generation capabilities.
   - [Link to Kaggle Notebook](https://www.kaggle.com/code/isratjahankhan/fine-tuning-notebook)
  
**Note:** The outputs from the second notebook were not saved due to the time-intensive process of handling 50,000 images from the CIFAR-10 dataset in each epoch. To improve processing efficiency, a "KeyboardInterrupt" is used after two epochs to save the output images. Since this is a public notebook, the "KeyboardInterrupt" error appearing in the output monitor may not be ideal. Therefore, only the code has been included without the output images.


## Requirements

To run the notebooks in this repository, the following software and libraries are required:

- **Python 3.7+**
- **Google Colab** (recommended for cloud execution) or **Kaggle**
- **GPU** (recommended for faster execution, though the model can also run on CPU)
- The following Python libraries:
  - `torch` (for tensor operations and model handling)
  - `torchvision` (for dataset manipulation and transformations)
  - `diffusers` (for working with Stable Diffusion models)
  - `PIL` (for image handling and transformations)
  - `google.colab` (for mounting Google Drive in Colab)

### Installation

If running locally or on Colab environment, install the required libraries using `pip`.

## Setup

### 1. Mount Google Drive (In case of using Google Colab)

The images generated by the model are saved to Google Drive for easy access. We will need to mount Google Drive in Google Colab to store the output images.

### 2. Device Setup

The notebooks are designed to automatically detect whether a GPU or CPU is available. If a GPU is available, it will be used for faster processing.

## Workflow

### 1. Image Generation Notebook

This notebook focuses on generating new images based on the CIFAR-10 dataset by leveraging a pre-trained Stable Diffusion model.

- **Model**: The model used is a lightweight version of Stable Diffusion (`stabilityai/stable-diffusion-2-1-base`), which is optimized for faster image generation while maintaining high-quality results.
- **Dataset**: The CIFAR-10 dataset is used, consisting of 60,000 32x32 color images across 10 classes. Images are resized to 512x512 resolution for generation.
- **Generation Process**: A fixed prompt (e.g., "A realistic photo") is used to generate images based on the original CIFAR-10 images, with parameters such as `strength` and `guidance_scale` to control the generation.
- **Output**: The generated images are saved in a directory in Google Drive or Kaggle's output directory, and the labels from the CIFAR-10 dataset are displayed for reference.

### 2. Fine-Tuning Notebook

This notebook involves fine-tuning the pre-trained Stable Diffusion model on a custom dataset to improve its image generation capabilities.

**Key Steps Involved:**

1. **Initializing the Fine-Tuning Process**:  
   The `StableDiffusionFinetuner` class is initialized with parameters like the model name (`stabilityai/stable-diffusion-2-1-base`), number of epochs (10), batch size (4), and learning rate (1e-5).

2. **Setting the Device**:  
   It checks if a GPU is available and assigns the device accordingly (GPU if available, otherwise CPU).

3. **Loading the Pre-Trained Model**:  
   The `StableDiffusionPipeline` is loaded using `from_pretrained`, which loads the pre-trained weights for the specified model. It is then moved to the selected device (GPU/CPU).

4. **Preparing the Data**:  
   The CIFAR-10 dataset is used for fine-tuning, with a transformation pipeline that resizes the images to 512x512, converts them to tensors, and normalizes them using ImageNet mean and standard deviation values. A DataLoader is created to handle batching and shuffling of the dataset.

5. **Optimizing and Defining the Loss Function**:  
   AdamW is used as the optimizer for fine-tuning, and MSE (Mean Squared Error) loss is used as the loss function. The optimizer is applied to the `unet` parameters of the model.

6. **Generating Images During Training**:  
   During each epoch, the model generates images using the `image-to-image` transformation. The model takes an input image and generates a new version based on a prompt generated from the class label (e.g., "Enhanced, photorealistic version of a cat"). The strength parameter controls how much the input image is transformed, and the number of inference steps refines the generation.

7. **Calculating Loss and Updating Weights**:  
   The generated images are compared to the original images using the MSE loss function. Backpropagation is performed to compute gradients and update the weights of the model using the AdamW optimizer.

8. **Saving Generated Images**:  
   The generated images are saved after each batch to the specified output directory, and the loss is displayed to track training progress.

9. **Epoch Iteration**:  
   This process is repeated for a specified number of epochs (10 in your case). The model is progressively fine-tuned with each batch, and the loss is printed at the end of each epoch.

The fine-tuning process essentially adjusts the weights of the pre-trained Stable Diffusion model to improve its performance in generating images based on the CIFAR-10 dataset.

## Example Usage

After running the Image Generation Notebook, you can download and view the generated images saved in Google Drive. For the Fine-Tuning Notebook, follow these steps:

1. Use the CIFAR-10 dataset, which is already included and loaded in the notebook.
2. Run the fine-tuning script to adapt the pre-trained Stable Diffusion model on the CIFAR-10 dataset.
3. Save the fine-tuned model and use it for generating improved images based on the CIFAR-10 dataset.

## Output Images

Here are some of the generated images from the first notebook (**Image Generation Notebook**):

![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_2.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_5.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_8.jpg?raw=true)

Here are some of the generated images from the second notebook (**Fine-Tuning Notebook**):

![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0.jpg?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0_method1.png?raw=true)
![image alt](https://github.com/IsratIJK/Assessment-2-Nagorik/blob/main/Output-images/generated_image_0_method2.png?raw=true)



**Note**: The fine-tuning process described here does not include any advanced steps and is currently under construction. Additional features and improvements will be added in future updates.

